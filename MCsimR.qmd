---
title: "Monte Carlo Simulation: Comparing Linear, Ridge, and Lasso Regression"
format: html
editor: visual
---

## Objective

This Monte Carlo simulation compares the performance of three regression models: - Linear Regression - Ridge Regression - Lasso Regression

We simulate 30 datasets: - 10 datasets with no correlation between predictors $x_1$ and $x_2$ - 10 datasets with mild correlation - 10 datasets with high correlation

Models are evaluated using Mean Squared Error (MSE), and results are summarized by mean and standard deviation of MSE.

## Setup

```{r}
library(tidyverse)
library(glmnet)
library(broom)
set.seed(111)
```

## Data Generation Function

```{r}
generate_data <- function(n = 100, correlation = 0.0) {
  sigma <- matrix(c(1, correlation, correlation, 1), nrow = 2)
  predictors <- mvtnorm::rmvnorm(n, mean = c(0, 0), sigma = sigma, method = "eigen")
  x1 <- predictors[, 1]
  x2 <- predictors[, 2]
  noise <- rnorm(n)
  y <- 0 * x1 + 4 * x2 + noise
  tibble(x1 = x1, x2 = x2, y = y)
}
```

## Simulation Loop with `map`

```{r}
correlations <- tibble(
  label = c("No Corr", "Mild Corr", "High Corr"),
  rho = c(0.0, 0.5, 0.99)
)

models <- c("Linear", "Ridge", "Lasso")

simulate_one <- function(df, model_name) {
  split <- initial_split(df, prop = 0.7)
  train <- training(split)
  test <- testing(split)
  x_train <- as.matrix(select(train, x1, x2))
  x_test <- as.matrix(select(test, x1, x2))
  y_train <- train$y
  y_test <- test$y

  fit <- switch(model_name,
    "Linear" = lm(y ~ x1 + x2, data = train),
    "Ridge" = glmnet(x_train, y_train, alpha = 0, lambda = 0.1),
    "Lasso" = glmnet(x_train, y_train, alpha = 1, lambda = 0.1)
  )

  preds <- if (model_name == "Linear") {
    predict(fit, newdata = test)
  } else {
    predict(fit, newx = x_test, s = ifelse(model_name == "Ridge", 1, 0.1))
  }

  mse <- mean((y_test - preds)^2)
  tibble(Model = model_name, MSE = mse)
}

library(rsample)

# Run models on each dataset
results <- correlations %>%
  mutate(simulations = map(rho, ~ replicate(10, generate_data(correlation = .x), simplify = FALSE))) %>%
  unnest(simulations) %>%
  mutate(results = map(simulations, ~ map_dfr(models, function(model) simulate_one(., model)))) %>%
  unnest(results)
```

## Summary Table

```{r}
summary <- results %>%
  group_by(Model, label) %>%
  summarise(
    Mean_MSE = mean(MSE),
    SD_MSE = sd(MSE),
    .groups = 'drop'
  )

summary
```

## Visualization (Optional)

```{r}
ggplot(results, aes(x = label, y = MSE, fill = Model)) +
  geom_boxplot() +
  labs(title = "Model MSE by Predictor Correlation", x = "Correlation", y = "MSE") +
  theme_minimal()
```

## Statistical comparison of results

```{r}
#| fig-height: 7.5
#| fig-width: 15

alpha <- 0.05

# Fit the ANOVA model
model <- aov(MSE ~ label + Model, data = results)

plot(model)

hist(model$residuals)

# Perform Tukey's HSD for multiple comparisons with alpha 
tukey_results <- TukeyHSD(model, conf.level = 1 - alpha) 

# View the results
plot(tukey_results)
```

What if we don't have normal residuals?

```{r}

# Kruskal-Wallis for 'label'
kruskal.test(MSE ~ label, data = results)

# Kruskal-Wallis for 'Model'
kruskal.test(MSE ~ Model, data = results)

# OR install https://github.com/cran/ARTool

# Pairwise comparisons for 'label'
pairwise.wilcox.test(results$MSE, results$label, p.adjust.method = "holm")

```
